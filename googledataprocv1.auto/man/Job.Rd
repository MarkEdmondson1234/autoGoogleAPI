% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataproc_objects.R
\name{Job}
\alias{Job}
\title{Job Object}
\usage{
Job(Job.labels = NULL, hadoopJob = NULL, placement = NULL,
  status = NULL, driverControlFilesUri = NULL, scheduling = NULL,
  pigJob = NULL, hiveJob = NULL, labels = NULL,
  driverOutputResourceUri = NULL, sparkJob = NULL, statusHistory = NULL,
  sparkSqlJob = NULL, yarnApplications = NULL, pysparkJob = NULL,
  reference = NULL)
}
\arguments{
\item{Job.labels}{The \link{Job.labels} object or list of objects}

\item{hadoopJob}{Job is a Hadoop job}

\item{placement}{Required Job information, including how, when, and where to run the job}

\item{status}{Output-only The job status}

\item{driverControlFilesUri}{Output-only If present, the location of miscellaneous control files which may be used as part of job setup and handling}

\item{scheduling}{Optional Job scheduling configuration}

\item{pigJob}{Job is a Pig job}

\item{hiveJob}{Job is a Hive job}

\item{labels}{Optional The labels to associate with this job}

\item{driverOutputResourceUri}{Output-only A URI pointing to the location of the stdout of the job's driver program}

\item{sparkJob}{Job is a Spark job}

\item{statusHistory}{Output-only The previous job status}

\item{sparkSqlJob}{Job is a SparkSql job}

\item{yarnApplications}{Output-only The collection of YARN applications spun up by this job}

\item{pysparkJob}{Job is a Pyspark job}

\item{reference}{Optional The fully qualified reference to the job, which can be used to obtain the equivalent REST path of the job resource}
}
\value{
Job object
}
\description{
Job Object
}
\details{
Autogenerated via \code{\link[googleAuthR]{gar_create_api_objects}}
A Cloud Dataproc job resource.
}
\seealso{
Other Job functions: \code{\link{Job.labels}},
  \code{\link{projects.regions.jobs.patch}}
}
